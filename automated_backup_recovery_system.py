#!/usr/bin/env python3
"""
Automated Backup and Recovery System
Following Google Cloud/AWS best practices for enterprise backup and disaster recovery
"""

import asyncio
import json
import time
import hashlib
import shutil
import tarfile
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
from enum import Enum
# Cloud storage imports (optional)
try:
    import boto3
    from botocore.exceptions import NoCredentialsError, ClientError
    BOTO3_AVAILABLE = True
except ImportError:
    BOTO3_AVAILABLE = False
    print("Note: boto3 not available - cloud storage features disabled")

# Import production components
from integrated_workflow_system import IntegratedWorkflowSystem
from specialized_workflow_agent import SpecializedWorkflowAgent

class BackupStatus(Enum):
    SCHEDULED = "scheduled"
    IN_PROGRESS = "in_progress" 
    COMPLETED = "completed"
    FAILED = "failed"
    RESTORED = "restored"

class RecoveryType(Enum):
    POINT_IN_TIME = "point_in_time"
    FULL_RESTORE = "full_restore"
    PARTIAL_RESTORE = "partial_restore"
    DISASTER_RECOVERY = "disaster_recovery"

@dataclass
class BackupMetadata:
    backup_id: str
    timestamp: datetime
    backup_type: str
    size_bytes: int
    checksum: str
    status: BackupStatus
    retention_days: int
    storage_location: str
    components: List[str]
    compression_ratio: float = 1.0
    duration_seconds: float = 0.0
    error_message: Optional[str] = None

@dataclass
class RecoveryPoint:
    recovery_id: str
    backup_id: str
    timestamp: datetime
    recovery_type: RecoveryType
    components: List[str]
    target_location: str
    validation_status: str
    recovery_time_objective: timedelta
    recovery_point_objective: timedelta

class AutomatedBackupRecoverySystem:
    """Enterprise-grade backup and recovery system following Google/AWS practices"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or self._get_default_config()
        self.backup_metadata: List[BackupMetadata] = []
        self.recovery_points: List[RecoveryPoint] = []
        self.system = None
        self.agents: Dict[str, SpecializedWorkflowAgent] = {}
        
        # Backup storage paths
        self.local_backup_path = Path(self.config['local_backup_path'])
        self.local_backup_path.mkdir(parents=True, exist_ok=True)
        
        # Setup logging with structured format (Google Cloud compatible)
        logging.basicConfig(
            level=logging.INFO,
            format='{"timestamp": "%(asctime)s", "level": "%(levelname)s", "component": "backup_recovery", "message": "%(message)s"}',
            handlers=[
                logging.FileHandler('backup_recovery.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # Initialize cloud storage clients
        self.s3_client = None
        self.gcs_client = None
        self._initialize_cloud_storage()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """Default configuration following enterprise backup best practices"""
        return {
            'local_backup_path': './backups',
            'retention_policy': {
                'daily': 7,      # Keep daily backups for 7 days
                'weekly': 4,     # Keep weekly backups for 4 weeks  
                'monthly': 12,   # Keep monthly backups for 12 months
                'yearly': 7      # Keep yearly backups for 7 years
            },
            'backup_schedule': {
                'full_backup_interval_hours': 24,    # Full backup every 24 hours
                'incremental_interval_hours': 6,     # Incremental every 6 hours
                'log_backup_interval_minutes': 15    # Log backup every 15 minutes
            },
            'recovery_objectives': {
                'rto_minutes': 60,    # Recovery Time Objective: 1 hour
                'rpo_minutes': 15     # Recovery Point Objective: 15 minutes
            },
            'storage': {
                'compression_enabled': True,
                'encryption_enabled': True,
                'multipart_threshold_mb': 100,
                'cloud_storage_enabled': False,  # Set to True when configured
                's3_bucket': None,
                'gcs_bucket': None
            },
            'validation': {
                'checksum_algorithm': 'sha256',
                'integrity_check_enabled': True,
                'restore_validation_enabled': True
            }
        }
    
    def _initialize_cloud_storage(self):
        """Initialize cloud storage clients (AWS S3, Google Cloud Storage)"""
        if BOTO3_AVAILABLE:
            try:
                # Initialize S3 client if configured
                if self.config['storage'].get('s3_bucket'):
                    self.s3_client = boto3.client('s3')
                    self.logger.info("AWS S3 client initialized successfully")
            except (NoCredentialsError, Exception) as e:
                self.logger.warning(f"S3 client initialization failed: {e}")
        
        try:
            # Initialize GCS client if configured
            if self.config['storage'].get('gcs_bucket'):
                from google.cloud import storage
                self.gcs_client = storage.Client()
                self.logger.info("Google Cloud Storage client initialized successfully")
        except Exception as e:
            self.logger.warning(f"GCS client initialization failed: {e}")
    
    async def initialize(self):
        """Initialize backup and recovery system"""
        self.logger.info("üîß Initializing Automated Backup & Recovery System...")
        
        # Initialize core system
        self.system = IntegratedWorkflowSystem()
        
        # Initialize production agents for backup
        agent_configs = [
            ("backup_agent", "Primary backup operations agent"),
            ("recovery_agent", "Disaster recovery and restore agent"),
            ("validation_agent", "Backup validation and integrity checks")
        ]
        
        for agent_id, description in agent_configs:
            agent = SpecializedWorkflowAgent(agent_id)
            await agent.initialize()
            self.agents[agent_id] = agent
            self.logger.info(f"‚úÖ {agent_id} initialized: {description}")
        
        # Load existing backup metadata
        await self._load_backup_metadata()
        
        # Schedule automated backups
        await self._schedule_automated_backups()
        
        self.logger.info("‚úÖ Backup & Recovery System initialized successfully")
        return True
    
    async def create_backup(self, backup_type: str = "full", components: List[str] = None) -> BackupMetadata:
        """Create comprehensive backup with Google/AWS best practices"""
        backup_id = f"backup_{int(time.time())}_{backup_type}"
        timestamp = datetime.now()
        
        self.logger.info(f"üîÑ Starting {backup_type} backup: {backup_id}")
        
        # Default to all components if not specified
        if components is None:
            components = ["system_state", "agent_data", "workflow_events", "configuration", "logs"]
        
        backup_metadata = BackupMetadata(
            backup_id=backup_id,
            timestamp=timestamp,
            backup_type=backup_type,
            size_bytes=0,
            checksum="",
            status=BackupStatus.IN_PROGRESS,
            retention_days=self._calculate_retention_days(backup_type),
            storage_location="",
            components=components
        )
        
        try:
            start_time = time.time()
            
            # Create backup directory
            backup_dir = self.local_backup_path / backup_id
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            total_size = 0
            
            # Backup each component
            for component in components:
                component_size = await self._backup_component(component, backup_dir)
                total_size += component_size
                self.logger.info(f"üì¶ Backed up {component}: {component_size:,} bytes")
            
            # Create compressed archive
            archive_path = await self._create_compressed_archive(backup_dir, backup_id)
            
            # Calculate checksum
            checksum = await self._calculate_checksum(archive_path)
            
            # Update metadata
            backup_metadata.size_bytes = archive_path.stat().st_size
            backup_metadata.checksum = checksum
            backup_metadata.storage_location = str(archive_path)
            backup_metadata.compression_ratio = total_size / backup_metadata.size_bytes if backup_metadata.size_bytes > 0 else 1.0
            backup_metadata.duration_seconds = time.time() - start_time
            backup_metadata.status = BackupStatus.COMPLETED
            
            # Upload to cloud storage if configured
            if self.config['storage']['cloud_storage_enabled']:
                await self._upload_to_cloud(archive_path, backup_id)
            
            # Clean up temporary directory
            shutil.rmtree(backup_dir)
            
            # Store metadata
            self.backup_metadata.append(backup_metadata)
            await self._save_backup_metadata()
            
            # Clean up old backups based on retention policy
            await self._cleanup_old_backups()
            
            self.logger.info(f"‚úÖ Backup completed: {backup_id} ({backup_metadata.size_bytes:,} bytes, {backup_metadata.duration_seconds:.2f}s)")
            
        except Exception as e:
            backup_metadata.status = BackupStatus.FAILED
            backup_metadata.error_message = str(e)
            self.logger.error(f"‚ùå Backup failed: {backup_id} - {e}")
            
        return backup_metadata
    
    async def _backup_component(self, component: str, backup_dir: Path) -> int:
        """Backup individual system component"""
        component_dir = backup_dir / component
        component_dir.mkdir(exist_ok=True)
        total_size = 0
        
        if component == "system_state":
            # Backup system health and configuration
            health_data = await self.system.get_system_health()
            health_file = component_dir / "system_health.json"
            with open(health_file, 'w') as f:
                json.dump(health_data, f, indent=2, default=str)
            total_size += health_file.stat().st_size
            
        elif component == "agent_data":
            # Backup agent states and configurations
            for agent_id, agent in self.agents.items():
                agent_status = await agent.get_agent_status()
                agent_file = component_dir / f"{agent_id}_status.json"
                with open(agent_file, 'w') as f:
                    json.dump(agent_status, f, indent=2, default=str)
                total_size += agent_file.stat().st_size
                
        elif component == "workflow_events":
            # Backup workflow events and history
            events_file = component_dir / "workflow_events.json"
            # Simulate event data - in production this would come from event store
            events_data = {
                "events": [],
                "metadata": {"backup_timestamp": datetime.now().isoformat()},
                "total_events": 0
            }
            with open(events_file, 'w') as f:
                json.dump(events_data, f, indent=2, default=str)
            total_size += events_file.stat().st_size
            
        elif component == "configuration":
            # Backup system configuration
            config_file = component_dir / "system_config.json"
            with open(config_file, 'w') as f:
                json.dump(self.config, f, indent=2, default=str)
            total_size += config_file.stat().st_size
            
        elif component == "logs":
            # Backup recent log files
            log_files = ["backup_recovery.log", "production_monitor.log"]
            for log_file in log_files:
                if Path(log_file).exists():
                    shutil.copy2(log_file, component_dir / log_file)
                    total_size += (component_dir / log_file).stat().st_size
        
        return total_size
    
    async def _create_compressed_archive(self, source_dir: Path, backup_id: str) -> Path:
        """Create compressed tar.gz archive of backup data"""
        archive_path = self.local_backup_path / f"{backup_id}.tar.gz"
        
        with tarfile.open(archive_path, 'w:gz') as tar:
            tar.add(source_dir, arcname=backup_id)
        
        return archive_path
    
    async def _calculate_checksum(self, file_path: Path) -> str:
        """Calculate SHA256 checksum of backup file"""
        hash_sha256 = hashlib.sha256()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    
    async def _upload_to_cloud(self, file_path: Path, backup_id: str):
        """Upload backup to cloud storage (S3/GCS)"""
        try:
            # Upload to S3 if configured
            if self.s3_client and self.config['storage']['s3_bucket']:
                bucket = self.config['storage']['s3_bucket']
                key = f"backups/{backup_id}.tar.gz"
                self.s3_client.upload_file(str(file_path), bucket, key)
                self.logger.info(f"üì§ Uploaded to S3: s3://{bucket}/{key}")
            
            # Upload to GCS if configured
            if self.gcs_client and self.config['storage']['gcs_bucket']:
                bucket_name = self.config['storage']['gcs_bucket']
                bucket = self.gcs_client.bucket(bucket_name)
                blob = bucket.blob(f"backups/{backup_id}.tar.gz")
                blob.upload_from_filename(str(file_path))
                self.logger.info(f"üì§ Uploaded to GCS: gs://{bucket_name}/backups/{backup_id}.tar.gz")
                
        except Exception as e:
            self.logger.error(f"‚ùå Cloud upload failed: {e}")
    
    def _calculate_retention_days(self, backup_type: str) -> int:
        """Calculate retention period based on backup type and policy"""
        retention_policy = self.config['retention_policy']
        
        if backup_type == "full":
            return retention_policy['yearly'] * 365
        elif backup_type == "incremental":
            return retention_policy['weekly'] * 7
        elif backup_type == "log":
            return retention_policy['daily']
        else:
            return retention_policy['monthly'] * 30
    
    async def restore_from_backup(self, backup_id: str, recovery_type: RecoveryType = RecoveryType.FULL_RESTORE, 
                                target_location: str = None) -> RecoveryPoint:
        """Restore system from backup with validation"""
        recovery_id = f"recovery_{int(time.time())}_{backup_id}"
        timestamp = datetime.now()
        
        self.logger.info(f"üîÑ Starting {recovery_type.value} restore: {recovery_id}")
        
        # Find backup metadata
        backup_meta = None
        for backup in self.backup_metadata:
            if backup.backup_id == backup_id:
                backup_meta = backup
                break
        
        if not backup_meta:
            raise ValueError(f"Backup not found: {backup_id}")
        
        if backup_meta.status != BackupStatus.COMPLETED:
            raise ValueError(f"Backup is not in completed state: {backup_meta.status}")
        
        # Create recovery point
        recovery_point = RecoveryPoint(
            recovery_id=recovery_id,
            backup_id=backup_id,
            timestamp=timestamp,
            recovery_type=recovery_type,
            components=backup_meta.components,
            target_location=target_location or "./restored",
            validation_status="in_progress",
            recovery_time_objective=timedelta(minutes=self.config['recovery_objectives']['rto_minutes']),
            recovery_point_objective=timedelta(minutes=self.config['recovery_objectives']['rpo_minutes'])
        )
        
        try:
            # Verify backup integrity
            if not await self._verify_backup_integrity(backup_meta):
                raise ValueError("Backup integrity verification failed")
            
            # Extract backup archive
            restore_path = await self._extract_backup_archive(backup_meta, target_location)
            
            # Perform component-specific restore operations
            await self._restore_components(backup_meta.components, restore_path)
            
            # Validate restore
            if self.config['validation']['restore_validation_enabled']:
                validation_result = await self._validate_restore(recovery_point)
                recovery_point.validation_status = "passed" if validation_result else "failed"
            else:
                recovery_point.validation_status = "skipped"
            
            self.recovery_points.append(recovery_point)
            await self._save_recovery_metadata()
            
            self.logger.info(f"‚úÖ Restore completed: {recovery_id}")
            
        except Exception as e:
            recovery_point.validation_status = "failed"
            self.logger.error(f"‚ùå Restore failed: {recovery_id} - {e}")
            raise
        
        return recovery_point
    
    async def _verify_backup_integrity(self, backup_meta: BackupMetadata) -> bool:
        """Verify backup file integrity using checksum"""
        backup_path = Path(backup_meta.storage_location)
        
        if not backup_path.exists():
            self.logger.error(f"Backup file not found: {backup_path}")
            return False
        
        # Calculate current checksum
        current_checksum = await self._calculate_checksum(backup_path)
        
        # Compare with stored checksum
        if current_checksum != backup_meta.checksum:
            self.logger.error(f"Checksum mismatch - Expected: {backup_meta.checksum}, Got: {current_checksum}")
            return False
        
        self.logger.info(f"‚úÖ Backup integrity verified: {backup_meta.backup_id}")
        return True
    
    async def _extract_backup_archive(self, backup_meta: BackupMetadata, target_location: str = None) -> Path:
        """Extract backup archive to target location"""
        if target_location is None:
            target_location = f"./restored/{backup_meta.backup_id}"
        
        restore_path = Path(target_location)
        restore_path.mkdir(parents=True, exist_ok=True)
        
        backup_path = Path(backup_meta.storage_location)
        
        with tarfile.open(backup_path, 'r:gz') as tar:
            tar.extractall(restore_path)
        
        self.logger.info(f"üì¶ Extracted backup to: {restore_path}")
        return restore_path
    
    async def _restore_components(self, components: List[str], restore_path: Path):
        """Restore individual system components"""
        for component in components:
            component_path = restore_path / component
            if component_path.exists():
                self.logger.info(f"üîÑ Restoring component: {component}")
                
                if component == "system_state":
                    # Restore system configuration
                    health_file = component_path / "system_health.json"
                    if health_file.exists():
                        with open(health_file, 'r') as f:
                            health_data = json.load(f)
                        self.logger.info(f"üìã System health data restored: {len(health_data)} entries")
                
                elif component == "configuration":
                    # Restore system configuration
                    config_file = component_path / "system_config.json"
                    if config_file.exists():
                        with open(config_file, 'r') as f:
                            restored_config = json.load(f)
                        self.logger.info(f"‚öôÔ∏è Configuration restored: {len(restored_config)} settings")
                
                # Additional component restore logic would go here
                
            else:
                self.logger.warning(f"‚ö†Ô∏è Component path not found: {component_path}")
    
    async def _validate_restore(self, recovery_point: RecoveryPoint) -> bool:
        """Validate restored system functionality"""
        try:
            # Test system initialization
            test_system = IntegratedWorkflowSystem()
            health = await test_system.get_system_health()
            
            if health.get('status') != 'healthy':
                self.logger.error("System health check failed after restore")
                return False
            
            # Test workflow execution
            test_result = await test_system.execute_workflow(
                "restore_validation_test",
                "validation",
                ["initialize", "validate"],
                {"restore_test": True}
            )
            
            if test_result.get('status') != 'success':
                self.logger.error("Workflow execution test failed after restore")
                return False
            
            self.logger.info("‚úÖ Restore validation passed")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Restore validation failed: {e}")
            return False
    
    async def _cleanup_old_backups(self):
        """Clean up old backups based on retention policy"""
        now = datetime.now()
        backups_to_remove = []
        
        for backup in self.backup_metadata:
            if backup.status != BackupStatus.COMPLETED:
                continue
                
            retention_date = backup.timestamp + timedelta(days=backup.retention_days)
            
            if now > retention_date:
                backups_to_remove.append(backup)
        
        for backup in backups_to_remove:
            try:
                # Remove local backup file
                backup_path = Path(backup.storage_location)
                if backup_path.exists():
                    backup_path.unlink()
                    self.logger.info(f"üóëÔ∏è Removed expired backup: {backup.backup_id}")
                
                # Remove from metadata
                self.backup_metadata.remove(backup)
                
            except Exception as e:
                self.logger.error(f"‚ùå Failed to remove backup {backup.backup_id}: {e}")
        
        if backups_to_remove:
            await self._save_backup_metadata()
    
    async def _schedule_automated_backups(self):
        """Schedule automated backup tasks"""
        self.logger.info("üìÖ Scheduling automated backups...")
        
        # Schedule full backup every 24 hours
        full_backup_interval = self.config['backup_schedule']['full_backup_interval_hours'] * 3600
        
        # Schedule incremental backup every 6 hours
        incremental_interval = self.config['backup_schedule']['incremental_interval_hours'] * 3600
        
        # Schedule log backup every 15 minutes
        log_interval = self.config['backup_schedule']['log_backup_interval_minutes'] * 60
        
        # In production, these would be scheduled with cron or task scheduler
        self.logger.info(f"üìã Full backup scheduled every {full_backup_interval/3600} hours")
        self.logger.info(f"üìã Incremental backup scheduled every {incremental_interval/3600} hours") 
        self.logger.info(f"üìã Log backup scheduled every {log_interval/60} minutes")
    
    async def _save_backup_metadata(self):
        """Save backup metadata to persistent storage"""
        metadata_file = self.local_backup_path / "backup_metadata.json"
        metadata_data = [asdict(backup) for backup in self.backup_metadata]
        
        # Convert datetime objects to ISO format
        for backup_data in metadata_data:
            backup_data['timestamp'] = backup_data['timestamp'].isoformat()
        
        with open(metadata_file, 'w') as f:
            json.dump(metadata_data, f, indent=2, default=str)
    
    async def _load_backup_metadata(self):
        """Load existing backup metadata from storage"""
        metadata_file = self.local_backup_path / "backup_metadata.json"
        
        if metadata_file.exists():
            try:
                with open(metadata_file, 'r') as f:
                    metadata_data = json.load(f)
                
                for backup_data in metadata_data:
                    backup_data['timestamp'] = datetime.fromisoformat(backup_data['timestamp'])
                    backup_data['status'] = BackupStatus(backup_data['status'])
                    self.backup_metadata.append(BackupMetadata(**backup_data))
                
                self.logger.info(f"üìã Loaded {len(self.backup_metadata)} backup metadata entries")
                
            except Exception as e:
                self.logger.error(f"‚ùå Failed to load backup metadata: {e}")
    
    async def _save_recovery_metadata(self):
        """Save recovery metadata to persistent storage"""
        metadata_file = self.local_backup_path / "recovery_metadata.json"
        metadata_data = []
        
        for recovery in self.recovery_points:
            recovery_data = asdict(recovery)
            recovery_data['timestamp'] = recovery_data['timestamp'].isoformat()
            recovery_data['recovery_time_objective'] = str(recovery_data['recovery_time_objective'])
            recovery_data['recovery_point_objective'] = str(recovery_data['recovery_point_objective'])
            recovery_data['recovery_type'] = recovery_data['recovery_type'].value
            metadata_data.append(recovery_data)
        
        with open(metadata_file, 'w') as f:
            json.dump(metadata_data, f, indent=2, default=str)
    
    async def get_backup_status(self) -> Dict[str, Any]:
        """Get comprehensive backup system status"""
        total_backups = len(self.backup_metadata)
        completed_backups = len([b for b in self.backup_metadata if b.status == BackupStatus.COMPLETED])
        failed_backups = len([b for b in self.backup_metadata if b.status == BackupStatus.FAILED])
        
        total_size = sum(b.size_bytes for b in self.backup_metadata if b.status == BackupStatus.COMPLETED)
        
        return {
            'timestamp': datetime.now().isoformat(),
            'system_status': 'operational',
            'total_backups': total_backups,
            'completed_backups': completed_backups,
            'failed_backups': failed_backups,
            'success_rate': (completed_backups / total_backups * 100) if total_backups > 0 else 100,
            'total_backup_size_gb': total_size / (1024**3),
            'retention_policy': self.config['retention_policy'],
            'recovery_objectives': self.config['recovery_objectives'],
            'cloud_storage_enabled': self.config['storage']['cloud_storage_enabled'],
            'recent_backups': [
                {
                    'backup_id': b.backup_id,
                    'timestamp': b.timestamp.isoformat(),
                    'type': b.backup_type,
                    'status': b.status.value,
                    'size_gb': b.size_bytes / (1024**3)
                }
                for b in sorted(self.backup_metadata, key=lambda x: x.timestamp, reverse=True)[:5]
            ]
        }

async def main():
    """Main backup and recovery system entry point"""
    backup_system = AutomatedBackupRecoverySystem()
    
    try:
        # Initialize backup system
        await backup_system.initialize()
        
        # Create initial full backup
        print("\nüîÑ Creating initial full system backup...")
        backup_result = await backup_system.create_backup("full")
        
        print(f"‚úÖ Backup completed: {backup_result.backup_id}")
        print(f"   Size: {backup_result.size_bytes / (1024**2):.2f} MB")
        print(f"   Compression: {backup_result.compression_ratio:.2f}x")
        print(f"   Duration: {backup_result.duration_seconds:.2f}s")
        
        # Get system status
        status = await backup_system.get_backup_status()
        print(f"\nüìä Backup System Status:")
        print(f"   Total backups: {status['total_backups']}")
        print(f"   Success rate: {status['success_rate']:.1f}%")
        print(f"   Total size: {status['total_backup_size_gb']:.2f} GB")
        
        print("\nüèÜ AUTOMATED BACKUP & RECOVERY SYSTEM OPERATIONAL")
        
    except Exception as e:
        print(f"\n‚ùå Backup system error: {e}")

if __name__ == "__main__":
    asyncio.run(main())