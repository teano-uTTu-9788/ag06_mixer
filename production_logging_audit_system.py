#!/usr/bin/env python3
"""
Production Logging and Audit Trail System
Following Google Cloud/AWS best practices for enterprise logging, audit trails, and compliance
"""

import asyncio
import json
import time
import uuid
import logging
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, asdict
from pathlib import Path
from enum import Enum
import threading
from logging.handlers import RotatingFileHandler
# Logging imports (with fallbacks)
try:
    from pythonjsonlogger import jsonlogger
    JSONLOGGER_AVAILABLE = True
except ImportError:
    JSONLOGGER_AVAILABLE = False
    print("Note: pythonjsonlogger not available - using standard logging")

try:
    import structlog
    STRUCTLOG_AVAILABLE = True
except ImportError:
    STRUCTLOG_AVAILABLE = False
    print("Note: structlog not available - using standard logging")
from contextlib import contextmanager

# Import production components
from integrated_workflow_system import IntegratedWorkflowSystem
from specialized_workflow_agent import SpecializedWorkflowAgent

class LogLevel(Enum):
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"
    AUDIT = "AUDIT"

class AuditEventType(Enum):
    USER_ACTION = "user_action"
    SYSTEM_EVENT = "system_event"
    SECURITY_EVENT = "security_event"
    CONFIGURATION_CHANGE = "configuration_change"
    DATA_ACCESS = "data_access"
    WORKFLOW_EXECUTION = "workflow_execution"
    AGENT_OPERATION = "agent_operation"
    BACKUP_OPERATION = "backup_operation"

class ComplianceFramework(Enum):
    SOX = "sox"           # Sarbanes-Oxley
    HIPAA = "hipaa"       # Healthcare
    GDPR = "gdpr"         # European Data Protection
    SOC2 = "soc2"         # Security & Availability
    ISO27001 = "iso27001" # Information Security
    PCI_DSS = "pci_dss"   # Payment Card Industry

@dataclass
class AuditEvent:
    event_id: str
    timestamp: datetime
    event_type: AuditEventType
    user_id: Optional[str]
    session_id: Optional[str]
    correlation_id: str
    service_name: str
    action: str
    resource: str
    outcome: str  # SUCCESS, FAILURE, PARTIAL
    severity: LogLevel
    metadata: Dict[str, Any]
    compliance_tags: List[ComplianceFramework]
    data_classification: str  # PUBLIC, INTERNAL, CONFIDENTIAL, RESTRICTED
    retention_days: int
    checksum: str

@dataclass
class LogEntry:
    log_id: str
    timestamp: datetime
    level: LogLevel
    service: str
    component: str
    message: str
    correlation_id: str
    user_id: Optional[str]
    session_id: Optional[str]
    request_id: Optional[str]
    trace_id: Optional[str]
    span_id: Optional[str]
    metadata: Dict[str, Any]
    tags: List[str]
    environment: str

class ProductionLoggingSystem:
    """Enterprise-grade logging system following Google Cloud/AWS best practices"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or self._get_default_config()
        self.audit_events: List[AuditEvent] = []
        self.log_entries: List[LogEntry] = []
        self.correlation_context = threading.local()
        
        # Create logging directory structure
        self.logs_path = Path(self.config['logging']['base_path'])
        self.logs_path.mkdir(parents=True, exist_ok=True)
        
        # Initialize structured logging (Google Cloud compatible)
        self._initialize_structured_logging()
        
        # Initialize audit logging
        self._initialize_audit_logging()
        
        # Set up log rotation and archival
        self._setup_log_rotation()
        
        self.logger = structlog.get_logger(__name__)
        
    def _get_default_config(self) -> Dict[str, Any]:
        """Default configuration for production logging"""
        return {
            'logging': {
                'base_path': './logs',
                'structured_format': True,
                'json_format': True,
                'correlation_tracking': True,
                'distributed_tracing': True,
                'log_level': 'INFO',
                'console_output': True,
                'file_output': True,
                'max_file_size_mb': 100,
                'backup_count': 10,
                'environment': 'production'
            },
            'audit': {
                'enabled': True,
                'separate_audit_log': True,
                'encryption_enabled': True,
                'digital_signatures': True,
                'immutable_storage': True,
                'real_time_monitoring': True,
                'compliance_frameworks': ['SOX', 'SOC2', 'ISO27001'],
                'default_retention_days': 2555  # 7 years for compliance
            },
            'performance': {
                'async_logging': True,
                'batch_size': 100,
                'buffer_timeout_seconds': 5,
                'compression_enabled': True
            },
            'security': {
                'log_sanitization': True,
                'pii_detection': True,
                'sensitive_data_masking': True,
                'access_control': True,
                'integrity_verification': True
            },
            'compliance': {
                'data_classification_required': True,
                'audit_trail_completeness': True,
                'retention_policy_enforcement': True,
                'legal_hold_support': True
            }
        }
    
    def _initialize_structured_logging(self):
        """Initialize structured logging with Google Cloud/AWS compatible format"""
        
        # Configure structlog for structured logging if available
        if STRUCTLOG_AVAILABLE:
            structlog.configure(
                processors=[
                    structlog.stdlib.add_log_level,
                    structlog.stdlib.PositionalArgumentsFormatter(),
                    structlog.processors.TimeStamper(fmt="ISO"),
                    structlog.processors.StackInfoRenderer(),
                    structlog.processors.format_exc_info,
                    structlog.processors.UnicodeDecoder(),
                    structlog.processors.JSONRenderer()
                ],
                context_class=dict,
                logger_factory=structlog.stdlib.LoggerFactory(),
                wrapper_class=structlog.stdlib.BoundLogger,
                cache_logger_on_first_use=True,
            )
        
        # Set up file handler with rotation
        log_file = self.logs_path / "application.log"
        file_handler = RotatingFileHandler(
            log_file,
            maxBytes=self.config['logging']['max_file_size_mb'] * 1024 * 1024,
            backupCount=self.config['logging']['backup_count']
        )
        
        # JSON formatter for structured logs
        if JSONLOGGER_AVAILABLE:
            json_formatter = jsonlogger.JsonFormatter(
                '%(asctime)s %(name)s %(levelname)s %(message)s %(pathname)s %(lineno)d %(funcName)s %(correlation_id)s %(user_id)s %(session_id)s'
            )
        else:
            json_formatter = logging.Formatter(
                '{"timestamp": "%(asctime)s", "name": "%(name)s", "level": "%(levelname)s", "message": "%(message)s", "pathname": "%(pathname)s", "lineno": %(lineno)d, "funcName": "%(funcName)s"}'
            )
        file_handler.setFormatter(json_formatter)
        
        # Configure root logger
        root_logger = logging.getLogger()
        root_logger.setLevel(getattr(logging, self.config['logging']['log_level']))
        root_logger.addHandler(file_handler)
        
        # Add console handler if enabled
        if self.config['logging']['console_output']:
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(json_formatter)
            root_logger.addHandler(console_handler)
    
    def _initialize_audit_logging(self):
        """Initialize separate audit logging system"""
        if not self.config['audit']['enabled']:
            return
            
        # Create separate audit log file
        audit_log_file = self.logs_path / "audit.log"
        audit_handler = RotatingFileHandler(
            audit_log_file,
            maxBytes=self.config['logging']['max_file_size_mb'] * 1024 * 1024,
            backupCount=self.config['logging']['backup_count'] * 2  # Keep more audit logs
        )
        
        # Special audit formatter
        if JSONLOGGER_AVAILABLE:
            audit_formatter = jsonlogger.JsonFormatter(
                '%(asctime)s %(event_id)s %(event_type)s %(user_id)s %(action)s %(resource)s %(outcome)s %(severity)s %(checksum)s'
            )
        else:
            audit_formatter = logging.Formatter(
                '{"timestamp": "%(asctime)s", "event_id": "%(event_id)s", "event_type": "%(event_type)s", "user_id": "%(user_id)s", "action": "%(action)s", "resource": "%(resource)s", "outcome": "%(outcome)s", "severity": "%(severity)s", "checksum": "%(checksum)s"}'
            )
        audit_handler.setFormatter(audit_formatter)
        
        # Create dedicated audit logger
        self.audit_logger = logging.getLogger('audit')
        self.audit_logger.setLevel(logging.INFO)
        self.audit_logger.addHandler(audit_handler)
        self.audit_logger.propagate = False  # Don't propagate to root logger
    
    def _setup_log_rotation(self):
        """Set up automated log rotation and archival"""
        # Log rotation is handled by RotatingFileHandler
        # Additional archival logic would be implemented here for cloud storage
        pass
    
    @contextmanager
    def correlation_context(self, correlation_id: str = None, user_id: str = None, session_id: str = None):
        """Context manager for correlation tracking across operations"""
        if correlation_id is None:
            correlation_id = str(uuid.uuid4())
            
        # Store in thread-local storage
        old_correlation_id = getattr(self.correlation_context, 'correlation_id', None)
        old_user_id = getattr(self.correlation_context, 'user_id', None)
        old_session_id = getattr(self.correlation_context, 'session_id', None)
        
        self.correlation_context.correlation_id = correlation_id
        self.correlation_context.user_id = user_id
        self.correlation_context.session_id = session_id
        
        try:
            yield correlation_id
        finally:
            # Restore previous context
            self.correlation_context.correlation_id = old_correlation_id
            self.correlation_context.user_id = old_user_id
            self.correlation_context.session_id = old_session_id
    
    def _get_current_context(self) -> Dict[str, Optional[str]]:
        """Get current correlation context"""
        return {
            'correlation_id': getattr(self.correlation_context, 'correlation_id', str(uuid.uuid4())),
            'user_id': getattr(self.correlation_context, 'user_id', None),
            'session_id': getattr(self.correlation_context, 'session_id', None)
        }
    
    def log(self, level: LogLevel, message: str, component: str = "system", 
            metadata: Dict[str, Any] = None, tags: List[str] = None, 
            request_id: str = None, trace_id: str = None, span_id: str = None):
        """Log structured message with full context"""
        
        context = self._get_current_context()
        
        log_entry = LogEntry(
            log_id=str(uuid.uuid4()),
            timestamp=datetime.utcnow(),
            level=level,
            service="ag06_workflow",
            component=component,
            message=self._sanitize_message(message),
            correlation_id=context['correlation_id'],
            user_id=context['user_id'],
            session_id=context['session_id'],
            request_id=request_id,
            trace_id=trace_id,
            span_id=span_id,
            metadata=metadata or {},
            tags=tags or [],
            environment=self.config['logging']['environment']
        )
        
        # Log using structlog or fallback
        log_data = asdict(log_entry)
        log_data['timestamp'] = log_data['timestamp'].isoformat()
        log_data['level'] = log_data['level'].value
        
        if STRUCTLOG_AVAILABLE:
            logger = structlog.get_logger(component)
            getattr(logger, level.value.lower())(message, **log_data)
        else:
            logger = logging.getLogger(component)
            getattr(logger, level.value.lower())(f"{message} | {json.dumps(log_data, default=str)}")
        
        # Store for potential batch processing
        self.log_entries.append(log_entry)
        
        # Trim old entries to prevent memory growth
        if len(self.log_entries) > 10000:
            self.log_entries = self.log_entries[-5000:]
    
    def audit(self, event_type: AuditEventType, action: str, resource: str, 
              outcome: str = "SUCCESS", severity: LogLevel = LogLevel.AUDIT,
              metadata: Dict[str, Any] = None, compliance_tags: List[ComplianceFramework] = None,
              data_classification: str = "INTERNAL", retention_days: int = None):
        """Create audit trail entry with compliance support"""
        
        if not self.config['audit']['enabled']:
            return
            
        context = self._get_current_context()
        
        audit_event = AuditEvent(
            event_id=str(uuid.uuid4()),
            timestamp=datetime.utcnow(),
            event_type=event_type,
            user_id=context['user_id'],
            session_id=context['session_id'],
            correlation_id=context['correlation_id'],
            service_name="ag06_workflow",
            action=action,
            resource=resource,
            outcome=outcome,
            severity=severity,
            metadata=self._sanitize_audit_metadata(metadata or {}),
            compliance_tags=compliance_tags or [ComplianceFramework.SOC2],
            data_classification=data_classification,
            retention_days=retention_days or self.config['audit']['default_retention_days'],
            checksum=""
        )
        
        # Calculate integrity checksum
        audit_event.checksum = self._calculate_audit_checksum(audit_event)
        
        # Log to dedicated audit logger
        audit_data = asdict(audit_event)
        audit_data['timestamp'] = audit_data['timestamp'].isoformat()
        audit_data['event_type'] = audit_data['event_type'].value
        audit_data['severity'] = audit_data['severity'].value
        audit_data['compliance_tags'] = [tag.value for tag in audit_data['compliance_tags']]
        
        self.audit_logger.info("AUDIT_EVENT", extra=audit_data)
        
        # Store for potential compliance reporting
        self.audit_events.append(audit_event)
        
        # Real-time monitoring alert for critical events
        if severity in [LogLevel.ERROR, LogLevel.CRITICAL] or outcome == "FAILURE":
            self._trigger_audit_alert(audit_event)
    
    def _sanitize_message(self, message: str) -> str:
        """Sanitize log message to remove PII and sensitive data"""
        if not self.config['security']['log_sanitization']:
            return message
            
        # Simple PII detection and masking
        import re
        
        # Mask email addresses
        message = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '***@***.***', message)
        
        # Mask credit card numbers
        message = re.sub(r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b', '****-****-****-****', message)
        
        # Mask phone numbers
        message = re.sub(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', '***-***-****', message)
        
        # Mask social security numbers
        message = re.sub(r'\b\d{3}-\d{2}-\d{4}\b', '***-**-****', message)
        
        return message
    
    def _sanitize_audit_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Sanitize audit metadata for compliance"""
        sanitized = {}
        sensitive_keys = ['password', 'token', 'key', 'secret', 'credential']
        
        for key, value in metadata.items():
            if any(sensitive in key.lower() for sensitive in sensitive_keys):
                sanitized[key] = "***REDACTED***"
            elif isinstance(value, str):
                sanitized[key] = self._sanitize_message(value)
            else:
                sanitized[key] = value
                
        return sanitized
    
    def _calculate_audit_checksum(self, audit_event: AuditEvent) -> str:
        """Calculate integrity checksum for audit event"""
        # Create deterministic string representation
        checksum_data = {
            'event_id': audit_event.event_id,
            'timestamp': audit_event.timestamp.isoformat(),
            'event_type': audit_event.event_type.value,
            'user_id': audit_event.user_id,
            'action': audit_event.action,
            'resource': audit_event.resource,
            'outcome': audit_event.outcome
        }
        
        checksum_string = json.dumps(checksum_data, sort_keys=True)
        return hashlib.sha256(checksum_string.encode()).hexdigest()
    
    def _trigger_audit_alert(self, audit_event: AuditEvent):
        """Trigger real-time alert for critical audit events"""
        alert_data = {
            'alert_type': 'AUDIT_ALERT',
            'event_id': audit_event.event_id,
            'severity': audit_event.severity.value,
            'action': audit_event.action,
            'resource': audit_event.resource,
            'outcome': audit_event.outcome,
            'user_id': audit_event.user_id,
            'timestamp': audit_event.timestamp.isoformat()
        }
        
        # In production, this would integrate with alerting systems (PagerDuty, Slack, etc.)
        self.log(LogLevel.CRITICAL, f"AUDIT ALERT: {audit_event.action} on {audit_event.resource} - {audit_event.outcome}", 
                "audit_monitor", metadata=alert_data, tags=["audit_alert", "security"])
    
    async def search_audit_trail(self, filters: Dict[str, Any], start_date: datetime = None, 
                                end_date: datetime = None, limit: int = 100) -> List[AuditEvent]:
        """Search audit trail with filtering and compliance support"""
        
        # Audit the audit search itself
        self.audit(
            AuditEventType.DATA_ACCESS,
            "audit_trail_search",
            "audit_events",
            metadata={
                'filters': filters,
                'start_date': start_date.isoformat() if start_date else None,
                'end_date': end_date.isoformat() if end_date else None,
                'limit': limit
            }
        )
        
        # Filter audit events
        results = []
        for event in self.audit_events:
            # Date range filter
            if start_date and event.timestamp < start_date:
                continue
            if end_date and event.timestamp > end_date:
                continue
                
            # Apply filters
            matches = True
            for key, value in filters.items():
                if hasattr(event, key):
                    event_value = getattr(event, key)
                    if isinstance(event_value, Enum):
                        event_value = event_value.value
                    if event_value != value:
                        matches = False
                        break
                        
            if matches:
                results.append(event)
                
            if len(results) >= limit:
                break
                
        return results
    
    async def generate_compliance_report(self, framework: ComplianceFramework, 
                                       start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Generate compliance report for specific framework"""
        
        self.audit(
            AuditEventType.SYSTEM_EVENT,
            "compliance_report_generation",
            f"compliance_framework_{framework.value}",
            metadata={
                'framework': framework.value,
                'start_date': start_date.isoformat(),
                'end_date': end_date.isoformat()
            },
            compliance_tags=[framework],
            data_classification="CONFIDENTIAL"
        )
        
        # Filter events for specific compliance framework
        compliance_events = [
            event for event in self.audit_events
            if framework in event.compliance_tags
            and start_date <= event.timestamp <= end_date
        ]
        
        # Generate report
        report = {
            'report_id': str(uuid.uuid4()),
            'framework': framework.value,
            'period': {
                'start_date': start_date.isoformat(),
                'end_date': end_date.isoformat()
            },
            'generated_at': datetime.utcnow().isoformat(),
            'total_events': len(compliance_events),
            'event_types': {},
            'outcomes': {},
            'severity_distribution': {},
            'user_activity': {},
            'resource_access': {},
            'compliance_summary': {}
        }
        
        # Analyze events
        for event in compliance_events:
            # Event types
            event_type = event.event_type.value
            report['event_types'][event_type] = report['event_types'].get(event_type, 0) + 1
            
            # Outcomes
            report['outcomes'][event.outcome] = report['outcomes'].get(event.outcome, 0) + 1
            
            # Severity distribution
            severity = event.severity.value
            report['severity_distribution'][severity] = report['severity_distribution'].get(severity, 0) + 1
            
            # User activity
            if event.user_id:
                report['user_activity'][event.user_id] = report['user_activity'].get(event.user_id, 0) + 1
            
            # Resource access
            report['resource_access'][event.resource] = report['resource_access'].get(event.resource, 0) + 1
        
        # Compliance summary
        total_events = len(compliance_events)
        success_events = report['outcomes'].get('SUCCESS', 0)
        failure_events = report['outcomes'].get('FAILURE', 0)
        
        report['compliance_summary'] = {
            'success_rate': (success_events / total_events * 100) if total_events > 0 else 100,
            'failure_rate': (failure_events / total_events * 100) if total_events > 0 else 0,
            'critical_events': report['severity_distribution'].get('CRITICAL', 0),
            'total_users': len(report['user_activity']),
            'total_resources': len(report['resource_access']),
            'compliance_score': self._calculate_compliance_score(report, framework)
        }
        
        return report
    
    def _calculate_compliance_score(self, report: Dict[str, Any], framework: ComplianceFramework) -> float:
        """Calculate compliance score based on framework requirements"""
        
        # Framework-specific scoring logic
        if framework == ComplianceFramework.SOX:
            # SOX requires complete audit trail and data integrity
            base_score = 100.0
            
            # Deduct for failures
            failure_rate = report['compliance_summary']['failure_rate']
            base_score -= failure_rate * 0.5
            
            # Deduct for missing audit events
            if report['total_events'] == 0:
                base_score -= 50.0
                
            return max(0.0, min(100.0, base_score))
            
        elif framework == ComplianceFramework.SOC2:
            # SOC2 focuses on security, availability, processing integrity
            base_score = 100.0
            
            # Security events weight
            security_events = report['event_types'].get('security_event', 0)
            total_events = report['total_events']
            security_ratio = security_events / total_events if total_events > 0 else 0
            
            # Higher security event ratio is good
            base_score += security_ratio * 10
            
            # Deduct for critical failures
            critical_events = report['compliance_summary']['critical_events']
            base_score -= critical_events * 2
            
            return max(0.0, min(100.0, base_score))
            
        else:
            # Generic scoring for other frameworks
            success_rate = report['compliance_summary']['success_rate']
            return success_rate
    
    async def export_audit_logs(self, output_format: str = "json", 
                               start_date: datetime = None, end_date: datetime = None,
                               compliance_framework: ComplianceFramework = None) -> str:
        """Export audit logs in various formats for compliance"""
        
        # Filter events
        events_to_export = []
        for event in self.audit_events:
            if start_date and event.timestamp < start_date:
                continue
            if end_date and event.timestamp > end_date:
                continue
            if compliance_framework and compliance_framework not in event.compliance_tags:
                continue
                
            events_to_export.append(event)
        
        # Audit the export itself
        self.audit(
            AuditEventType.DATA_ACCESS,
            "audit_log_export",
            "audit_events",
            metadata={
                'output_format': output_format,
                'events_exported': len(events_to_export),
                'compliance_framework': compliance_framework.value if compliance_framework else None
            },
            data_classification="CONFIDENTIAL"
        )
        
        # Generate export
        if output_format.lower() == "json":
            export_data = []
            for event in events_to_export:
                event_data = asdict(event)
                event_data['timestamp'] = event_data['timestamp'].isoformat()
                event_data['event_type'] = event_data['event_type'].value
                event_data['severity'] = event_data['severity'].value
                event_data['compliance_tags'] = [tag.value for tag in event_data['compliance_tags']]
                export_data.append(event_data)
                
            export_filename = f"audit_export_{int(time.time())}.json"
            export_path = self.logs_path / export_filename
            
            with open(export_path, 'w') as f:
                json.dump(export_data, f, indent=2, default=str)
                
            return str(export_path)
            
        elif output_format.lower() == "csv":
            import csv
            export_filename = f"audit_export_{int(time.time())}.csv"
            export_path = self.logs_path / export_filename
            
            with open(export_path, 'w', newline='') as csvfile:
                if events_to_export:
                    fieldnames = ['event_id', 'timestamp', 'event_type', 'user_id', 'action', 
                                 'resource', 'outcome', 'severity', 'correlation_id']
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    
                    for event in events_to_export:
                        writer.writerow({
                            'event_id': event.event_id,
                            'timestamp': event.timestamp.isoformat(),
                            'event_type': event.event_type.value,
                            'user_id': event.user_id,
                            'action': event.action,
                            'resource': event.resource,
                            'outcome': event.outcome,
                            'severity': event.severity.value,
                            'correlation_id': event.correlation_id
                        })
            
            return str(export_path)
        
        else:
            raise ValueError(f"Unsupported export format: {output_format}")
    
    async def get_logging_system_status(self) -> Dict[str, Any]:
        """Get comprehensive logging system status"""
        
        # Calculate statistics
        total_logs = len(self.log_entries)
        total_audits = len(self.audit_events)
        
        log_levels = {}
        for entry in self.log_entries[-1000:]:  # Last 1000 entries
            level = entry.level.value
            log_levels[level] = log_levels.get(level, 0) + 1
        
        audit_outcomes = {}
        for event in self.audit_events[-1000:]:  # Last 1000 events
            outcome = event.outcome
            audit_outcomes[outcome] = audit_outcomes.get(outcome, 0) + 1
        
        # Get disk usage
        logs_size = sum(f.stat().st_size for f in self.logs_path.rglob('*.log') if f.is_file())
        
        return {
            'timestamp': datetime.utcnow().isoformat(),
            'system_status': 'operational',
            'logging': {
                'total_log_entries': total_logs,
                'log_level_distribution': log_levels,
                'structured_logging': self.config['logging']['structured_format'],
                'json_format': self.config['logging']['json_format'],
                'log_rotation_enabled': True
            },
            'audit': {
                'enabled': self.config['audit']['enabled'],
                'total_audit_events': total_audits,
                'outcome_distribution': audit_outcomes,
                'compliance_frameworks': self.config['audit']['compliance_frameworks'],
                'retention_days': self.config['audit']['default_retention_days']
            },
            'storage': {
                'logs_path': str(self.logs_path),
                'total_size_mb': logs_size / (1024 * 1024),
                'compression_enabled': self.config['performance']['compression_enabled']
            },
            'security': {
                'log_sanitization': self.config['security']['log_sanitization'],
                'pii_detection': self.config['security']['pii_detection'],
                'integrity_verification': self.config['security']['integrity_verification']
            },
            'performance': {
                'async_logging': self.config['performance']['async_logging'],
                'batch_size': self.config['performance']['batch_size']
            }
        }

async def main():
    """Main production logging system entry point"""
    logging_system = ProductionLoggingSystem()
    
    try:
        print("\nüîß Production Logging & Audit System Initialization")
        
        # Test structured logging
        with logging_system.correlation_context(user_id="system", session_id="init"):
            logging_system.log(LogLevel.INFO, "Production logging system started", "initialization")
            
            # Test audit logging
            logging_system.audit(
                AuditEventType.SYSTEM_EVENT,
                "system_initialization",
                "logging_system",
                outcome="SUCCESS",
                metadata={"version": "1.0", "environment": "production"},
                compliance_tags=[ComplianceFramework.SOC2, ComplianceFramework.ISO27001]
            )
            
            # Test workflow audit
            logging_system.audit(
                AuditEventType.WORKFLOW_EXECUTION,
                "test_workflow",
                "workflow_engine",
                outcome="SUCCESS",
                metadata={"workflow_id": "test_001", "duration_ms": 1250}
            )
            
            # Test security event
            logging_system.audit(
                AuditEventType.SECURITY_EVENT,
                "authentication_attempt",
                "auth_service",
                outcome="SUCCESS",
                severity=LogLevel.WARNING,
                compliance_tags=[ComplianceFramework.SOX]
            )
        
        # Get system status
        status = await logging_system.get_logging_system_status()
        print(f"\nüìä Logging System Status:")
        print(f"   Total log entries: {status['logging']['total_log_entries']}")
        print(f"   Total audit events: {status['audit']['total_audit_events']}")
        print(f"   Storage size: {status['storage']['total_size_mb']:.2f} MB")
        print(f"   Structured logging: {status['logging']['structured_logging']}")
        print(f"   Audit enabled: {status['audit']['enabled']}")
        
        # Generate sample compliance report
        end_date = datetime.utcnow()
        start_date = end_date - timedelta(days=1)
        
        report = await logging_system.generate_compliance_report(
            ComplianceFramework.SOC2, start_date, end_date
        )
        print(f"\nüìã SOC2 Compliance Report:")
        print(f"   Compliance score: {report['compliance_summary']['compliance_score']:.1f}%")
        print(f"   Success rate: {report['compliance_summary']['success_rate']:.1f}%")
        print(f"   Total events: {report['total_events']}")
        
        print("\nüèÜ PRODUCTION LOGGING & AUDIT SYSTEM OPERATIONAL")
        
    except Exception as e:
        print(f"\n‚ùå Logging system error: {e}")

if __name__ == "__main__":
    asyncio.run(main())